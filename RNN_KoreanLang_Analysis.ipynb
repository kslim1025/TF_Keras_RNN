{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_KoreanLang_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwv3O0w9U7ozr6fxp5+Tlc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kslim1025/TF_Keras_RNN/blob/master/RNN_KoreanLang_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X33UMrVR1Nj",
        "outputId": "0408a8ef-eb4c-41f6-9cc4-5240f7e97b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# MNIST 문자열 분석으로 유명한 데이터셋이 패션에 관한 데이터 세트를 만듬 그게 FashionMINIST 데이터셋이다.\n",
        "# 데이터 이미지가 0에서 255까지 값을 가지는 28x28이미지라는 것을 확인가능\n",
        "# 정답이 되는 라벨을 확인하기 위해 print를 붙여서  확인\n",
        "# 외부 데이터를 이용한 정제과정\n",
        "# ctrl+enter를 사용한 런타임가능\n",
        "# !nvidia-smi : 어떤 GPU를 사용하는지 확인가능한 명령어 \n",
        "#\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# 넘파이는 수학과 과학 연산에 특화된 파이썬 모듈로 딥러닝에서도 유용하게 사용된다.\n",
        "\n",
        "import numpy as np;\n",
        "import tensorflow as tf;\n",
        "import pandas as pd;\n",
        "import matplotlib.pyplot as plt;\n",
        "import math;\n",
        "import re;\n",
        "################################################################################\n",
        "!nvidia-smi\n",
        "       \n",
        "#+-----------------------------------------------------------------------------+\n",
        "#| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
        "#|-------------------------------+----------------------+----------------------+\n",
        "#| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "#| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "#|                               |                      |               MIG M. |\n",
        "#|===============================+======================+======================|\n",
        "#|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
        "#| N/A   51C    P0    35W / 250W |   1581MiB / 16280MiB |      0%      Default |\n",
        "#|                               |                      |                 ERR! |\n",
        "#+-------------------------------+----------------------+----------------------+\n",
        "#                                                                               \n",
        "#+-----------------------------------------------------------------------------+\n",
        "#| Processes:                                                                  |\n",
        "#|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
        "#|        ID   ID                                                   Usage      |\n",
        "#|=============================================================================|\n",
        "#|  No running processes found                                                 |\n",
        "#+-----------------------------------------------------------------------------+\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# 한글 원본 텍스트를자소 단위와 단어 단위로 나눠서 순환 신경망으로 생성\n",
        "# 이번에 작업할때 사용할 원본 텍스트는 국사 편찬위원회에서 제공하는 조선왕조실록 국문 번역본을 사용\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('input.txt', 'http://bit.ly/2Mc3SOV')\n",
        "\n",
        "#데이터를 메모리에 불러오기, 인코딩 형식은 utf-8로 지정\n",
        "train_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "#텍스트가 총 몇 자인지를 확인\n",
        "print('텍스트의 길이 : {} charaters'.format(len(train_text)))\n",
        "print()\n",
        "\n",
        "#처음에는 300자를 임시로 확인\n",
        "print(train_text[:300])\n",
        "\n",
        "#훈련 데이터입력 정제\n",
        "# from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "\n",
        "def clean_str(string):\n",
        "  string = re.sub(r\"[^가-힣A-Za-z0-9(),!?'\\']\",\" \",string)\n",
        "  string = re.sub(r\"\\'s\", \"\\'s\", string)\n",
        "  string = re.sub(r\"\\'ve\",\"\\'ve\",string)\n",
        "  string = re.sub(r\"n\\'t\", \"n\\'t\",string)\n",
        "  string = re.sub(r\"\\'re\",\"\\'re\",string)\n",
        "  string = re.sub(r\"\\'d\",\"\\'d\",string)\n",
        "  string = re.sub(r\"\\'ll\",\"\\'ll\",string)\n",
        "  string = re.sub(r\",\",\",\",string)\n",
        "  string = re.sub(r\"!\",\" ! \",string)\n",
        "  string = re.sub(r\"\\(\", \" \\( \",string)\n",
        "  string = re.sub(r\"\\)\", \" \\) \",string)\n",
        "  string = re.sub(r\"\\?\", \" \\? \",string)\n",
        "  string = re.sub(r\"\\?\", \" \\? \",string)\n",
        "  string = re.sub(r\"\\s{2,}\",\" \",string)\n",
        "  string = re.sub(r\"\\s{2,}\",\"\\'\",string)\n",
        "  string = re.sub(r\"\\'\",\"\",string)\n",
        "  \n",
        "  return string\n",
        "\n",
        "train_text = train_text.split('\\n')\n",
        "train_text = [clean_str(sentence) for sentence in train_text]\n",
        "train_text_X = []\n",
        "\n",
        "for sentence in train_text:\n",
        "  train_text_X.extend(sentence.split(' '))\n",
        "  train_text_X.append('\\n')\n",
        "\n",
        "train_text_X = [word for word in train_text_X if word != '']\n",
        "\n",
        "print(train_text_X[:20])\n",
        "\n",
        "#단어를 토큰화 작업\n",
        "#Tokenizer를 쓰면 계산시간이 늘기 때문에 직접 토큰화 진행\n",
        "\n",
        "#단어 set생성\n",
        "vocab = sorted(set(train_text_X))\n",
        "vocab.append('UNK')\n",
        "print('{} unique words '.format(len(vocab)))\n",
        "\n",
        "#vocab list를 숫자로 매핑하고, 반대도 실행합니다\n",
        "word2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2word = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([word2idx[c] for c in train_text_X])\n",
        "\n",
        "#word2idx의 일부를 알아보기 쉽게 출력\n",
        "print('{')\n",
        "for word, _ in zip(word2idx, range(10)):\n",
        "  print('{:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
        "print(' ...\\n}')\n",
        "#텍스트에 존재하지 않는 토큰을 나타내는 UNK 사용. 총 332640개의 단어중 UNK인덱스는 332639개\n",
        "print('index of UNK: {}'.format(word2idx['UNK']))\n",
        "\n",
        "print(train_text_X[:20])\n",
        "print(text_as_int[:20])\n",
        "\n",
        "# seq_length를 25로 설정해서 25개의 단어가 주어졌을때 다음 단어를 예측하도록 데이터를 만듬\n",
        "seq_length = 25\n",
        "examples_per_epoch = len(text_as_int)\n",
        "#기존에 사용했던 train_X,train_Y를 넘파이 array로 만드는 방식 말고 tf.data.Dataset을 사용\n",
        "#코드의 간편화, 데이터 섞기, 배치 수만큼 자르기, 매핑등의 장점\n",
        "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "#drop_remainder=True을 사용해서 남는 부분은 삭제\n",
        "#출력해서 의도한 대로 26개 단어 반환\n",
        "sentence_dataset = sentence_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in sentence_dataset.take(1):\n",
        "  print(idx2word[item.numpy()])\n",
        "  print(item.numpy())\n",
        "\n",
        "def split_input_target(chunk):\n",
        "  return [chunk[:-1], chunk[:-1]]\n",
        "\n",
        "train_dataset = sentence_dataset.map(split_input_target)\n",
        "for x,y in train_dataset.take(1):\n",
        "  print(idx2word[x.numpy()])\n",
        "  print(x.numpy())\n",
        "  print(idx2word[y.numpy()])\n",
        "  print(y.numpy())\n",
        "\n",
        "#단어 단위 생성 모델정의\n",
        "total_words = len(vocab)\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(total_words, 100, input_length=seq_length),\n",
        "                             tf.keras.layers.LSTM(units=100, return_sequences=True),\n",
        "                             tf.keras.layers.Dropout(0.2),\n",
        "                             tf.keras.layers.LSTM(units=100),\n",
        "                             tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#단어 단위 생성 모델 학습\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def testmodel(epoch, logs):\n",
        "  if epoch % 5 !=0 and epoch !=49:\n",
        "    return\n",
        "  test_sentence = train_text[0]\n",
        "\n",
        "  next_words=100\n",
        "  for _ in range(next_words):\n",
        "    test_text_X = text_sentence.split(' ')[-seq_length:]\n",
        "    test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
        "    test_text_X = pad_sequences([test_text_X], maxlen = seq_length, padding = 'pre', value=word2idx['UNK'])\n",
        "    output_idx = model.predict_classes(test_text_X)\n",
        "    test_sentence += ' ' + idx2word[output_idx[0]]\n",
        "\n",
        "  print()\n",
        "  print(test_sentence)\n",
        "  print()\n",
        "\n",
        "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
        "\n",
        "history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch = steps_per_epoch, callbacks=[testmodelcb], verbose=2)\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "test_sentence = '동대문에 나가 공무 후 활을 쐈다'\n",
        "\n",
        "next_words =100\n",
        "for _ in range(next_words):\n",
        "  test_text_X = test_sentence.split(' ')[-seq_length:]\n",
        "  test_text_X = np.array([word2idx[c] if c in word2idx else total_words+1 for c in test_text_X])\n",
        "  test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding = 'pre', value=word2idx['UNK'])\n",
        "\n",
        "  output_idx = model.predict_classes(test_text_X)\n",
        "  test_sentence += ' ' + idx2word[output_idx[0]]\n",
        "\n",
        "print(test_sentece)\n",
        "\n",
        "# 한글을 자소단위로 분리하고 다시 합칠 수 있는 라이브러리로 jamotools를 사용\n",
        "# !pip install jamotools\n",
        "\n",
        "import jamotools\n",
        "\n",
        "train_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "s = train_text[:100]\n",
        "print(s)\n",
        "\n",
        "#한글 텍스트를 자모 단위로 분리합니다. 한자 등에는 영향이 없습니다\n",
        "s_split = jamotools.split_syllabels(s)\n",
        "print(s_split)\n",
        "\n",
        "# 자모를 분리했다가 다시 합친 s2는 원래 텍스트인 s와 같다는 것을 두번째 출력인 True인 것에서 확인할 수 있다\n",
        "s2 = jamotools.join_jamos(s_split)\n",
        "print(s2)\n",
        "print(s == s2)\n",
        "\n",
        "#자모 토큰화\n",
        "#텍스트를 자모 단위로 나눕니다. 데이터가 크기 때문에 약간 시간이 걸립니다.\n",
        "train_text_X = jamotools.split_syllabels(train_text)\n",
        "vocab = sorted(set(train_text_X))\n",
        "vocab.append('UNK')\n",
        "print('{} unique charatoers'.format(len(vocab)))\n",
        "\n",
        "#vocab list를 숫자로 매핑 그리고 역으로도 실행\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "#word2idx의 일부를 알아보기 쉽게 출력해봅니다.\n",
        "print('{')\n",
        "for char, _ in zip(char2idx, range(10)):\n",
        "  print(' {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('.....\\n}')\n",
        "\n",
        "print('index of UNK: {}'.format(char2idx['UNK']))\n",
        "\n",
        "#토큰 데이터 확인\n",
        "print(train_text_X[:20])\n",
        "print(text_as_int[:20])\n",
        "\n",
        "#학습 데이터세트 생성\n",
        "seq_length = 80\n",
        "examples_per_epoch = len(text_as_int) //seq_length\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "char_dataset = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in char_dataset.take(1):\n",
        "  print(idx2char[item.numpy()])\n",
        "  print(item.numpy())\n",
        "\n",
        "def split_input_target(chunk):\n",
        "  return [chunk[:-1], chunk[-1]]\n",
        "\n",
        "train_dataset = char_dataset.map(split_input_target)\n",
        "for x,y in train_dataset.take(1):\n",
        "  print(idx2char[x.numpy()])\n",
        "  print(x.numpy())\n",
        "  print(idx2char[y.numpy()])\n",
        "  print(y.numpy())\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = examples_per_epoch //BATCH_SIZE\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "#자소 단위 생성 모델 정의\n",
        "total_chars = len(vocab)\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(total_chars, 100, input_length=seq_length),\n",
        "                             tf.keras.layers.LSTM(units=400),\n",
        "                             tf.keras.layers.Dense(total_chars, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adma', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "from tensorflw.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def testmodel(epoch, logs):\n",
        "  if epoch % 5 != 0 and epoch != 99:\n",
        "    return\n",
        "\n",
        "  test_sentence = train_text[:48]\n",
        "  test_sentence = jamotools.split_syllabels(test_sentence)\n",
        "\n",
        "  next_chars = 300\n",
        "  for _ in range(next_chars):\n",
        "    test_text_X = test_sentence[-seq_length:]\n",
        "    test_text_X = np.array([char2idx[c] if c in char2idx else char2idx['UNK'] for c in test_text_X])\n",
        "    test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding ='pre', value=char2idx['UNK'])\n",
        "\n",
        "    output_idx = model.predict_classes(test_text_X)\n",
        "    test_sentence += idx2char[output_idx[0]]\n",
        "\n",
        "  print()\n",
        "  print(jamotools.join_jamos(test_sentence))\n",
        "  print()\n",
        "\n",
        "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
        "\n",
        "history = model.fit(train_dataset.repeat(), epochs=100, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "텍스트의 길이 : 26265493 charaters\n",
            "\n",
            "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \n",
            "태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘(諱)는 단(旦)이요, 자(字)는 군진(君晉)이다. 그전의 휘(諱)는 이성계(李成桂)요, 호(號)는 송헌(松軒)이다. 전주(全州)의 대성(大姓)이다. 사공(司空) 휘(諱) 이한(李翰)이 신라(新羅)에 벼슬하여 태종왕(太宗王)001) 의 10대(代) 손자인 군윤(軍尹) 김은의(金殷義)의 딸에게 장가들어 시중(侍中) 휘(諱) 이자연(李自延)을 낳았다. 시중이 \n",
            "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n",
            "352275 unique words \n",
            "{\n",
            "'\\n':   0,\n",
            "'!' :   1,\n",
            "',' :   2,\n",
            "',광연전':   3,\n",
            "',동해':   4,\n",
            "',박강':   5,\n",
            "',사배':   6,\n",
            "',연':   7,\n",
            "',흥':   8,\n",
            "'001':   9,\n",
            " ...\n",
            "}\n",
            "index of UNK: 352274\n",
            "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n",
            "[317209 243792 172112  19043 118674 244465 266449 165427 239355  31342\n",
            " 202515 271742      0 317209  27699 290244  38903 174791 191902  89761]\n",
            "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
            " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '\\\\(' '\\\\)' '의' '성은' '이씨' '\\\\(']\n",
            "[317209 243792 172112  19043 118674 244465 266449 165427 239355  31342\n",
            " 202515 271742      0 317209  27699 290244  38903 174791 191902  89761\n",
            "  18950  18951 237978 175354 244420  18950]\n",
            "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
            " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '\\\\(' '\\\\)' '의' '성은' '이씨']\n",
            "[317209 243792 172112  19043 118674 244465 266449 165427 239355  31342\n",
            " 202515 271742      0 317209  27699 290244  38903 174791 191902  89761\n",
            "  18950  18951 237978 175354 244420]\n",
            "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
            " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '\\\\(' '\\\\)' '의' '성은' '이씨']\n",
            "[317209 243792 172112  19043 118674 244465 266449 165427 239355  31342\n",
            " 202515 271742      0 317209  27699 290244  38903 174791 191902  89761\n",
            "  18950  18951 237978 175354 244420]\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 25, 100)           35227500  \n",
            "_________________________________________________________________\n",
            "lstm_28 (LSTM)               (None, 25, 100)           80400     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 25, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_29 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 352275)            35579775  \n",
            "=================================================================\n",
            "Total params: 70,968,075\n",
            "Trainable params: 70,968,075\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9df6b59e5a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0mtestmodelcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambdaCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestmodelcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'steps_per_epoch' is not defined"
          ]
        }
      ]
    }
  ]
}