# TF_Keras_RNN
RNN (순환 신경망 구축)

1. SimpleRNN 레이어

 SimpleRNN 레이어에는 한가지 치명적인 단점이 있습니다. 입력데이터가 길어질수록, 데이터의 타임스텝이 커질수록 학습능력이 떨어집니다.
 이를 장기 의존성(Long-Term Dependency)문제라고 하며, 입력 데이터와 출력 사이의 길이가 멀어질 수록 연관 관계가 적어집니다.
 현재의 답을 얻기 위해 과거의 정보에 의존해야하는 RNN이지만 과거 시점이 현재와 너무 멀어지면 문제를 풀기 힘들어지는점이 있습니다.
 이러한 문제점을 개선하기 위해서 장기의존성 문제를 해결하기 위한 구조로 셉 호흐라이터(Sepp Hochreiter)와 유르겐 슈미트후버에 의해 LSTM이 1997년에 제안됩니다.

2. LSTM 레이어

 LSTM은 RNN에 비해 복잡한 구조를 가지고 있는데 가장 큰 특징은 출력외에 LSTM 셀사이에서만 공유되는 셀상태를 가지고 있다는점 입니다.
 LSTM 레이어에는 활성화 함수로 tanh외에 시그모이드 함수가 쓰였습니다. 시그모이드 함수는 항상 0~1 범위의 출력을 내며, 시그모이드 함수는 이러한 출력의 특성
 때문에 정보가 통과하는 게이트 역할을 합니다. 출력이 0이면 입력된 정보가 하나도 통과하지 못하는것이 1이면 100% 통과하는 형태로 진행됩니다

3. GRU 레이어

 GRU(Gated Recurrent Unit) 레이어는 뉴욕대학교의 조경현 교수등이 제안한 구조이다, 조경현 교수는 앞에서 나온 장기의존성에 관한 
 논문을 쓴 요슈아 벤지오 교수의 제자이며, GRU레이어는 LSTM레이어와 비슷한 역할을 하지만 구조가 더 간단하기 때문에 계산상 
 이점이 있고 어떤문제에서는 LSTM레이어보다 좋은 성능을 보이기도 합니다.

4. Embedding Layer(임베딩 레이어)

 자연어를 수치화된 정보로 바꾸기 위한 레이어입니다.

 자연어는 시간의 흐름에 따라 정보가 연속적으로 이어지는 시퀀스 데이터입니다.
 이미지를 픽셀 단위로 잘게 쪼갤 수 있듯이 자연어도 정보를 잘게 쪼갤 수 잇습니다. 영어는 문자(character), 한글은 문자를 넘어 자소 단위로도 쪼갤 수 있습니다.
 이보다 더 큰 단위로는 띄어쓰기 단위인 단어가 있습니다. 또 이 방법들과 다르게 몇개의 문자를 묶어서 파악하려는 n-gram 기법입니다. 예를 들어'this is it'
 이라는 문장을 3개의 문자를 묶은 3-gram으로 나타내려면 ['thi','his','is ','s i','is','is ','s i', ' it', 'it.']이라는 배열로 나타낼 수 있습니다.
 딥러닝 기법의 발달로 이후는 n-gram보다 단어나 문자 단위의 자연어 처리가 많이 쓰이고 있습니다.

 즉 Word Embedding > n-gram보다 좋은 성능을 냅니다.
 임베딩 레이어보다 좀 더 쉬운 기법은 자연어를 구성하는 단위에 대해 정수를 (index)를 저장하는 방법입니다.
 단어를 기반으로 정 수 인덱스를 저장하는 예를 들면 'this is a big cat'이라는 문장에 대해 정수 인덱스를 저장하면 처음 나오는 단어부터 인덱스를 저장합니다.

 임베딩 레이어에 대한 개념은 쉬운 편이지만 학습시키는 방법에는 여러가지가 있습니다. 대표적인 방법은 Word2Vec, GloVe, FastText, ELMo등이 있습니다.
 이 방법론으로 미리 훈련된 임베딩 레이어의 가중치를 불러와서 사용하면 시간을 절약할 수 있습니다.
 
 4.1 긍정/중립/부정 감정 분석
 
 긍정, 부정 감성 분석
 감성분석은 입력된 자연어 안의 주관적의견, 감정 등을 찾아낸다.
 이 가운데 극성(polarity) 감성 분석은 문장의 궁정/부정이나 긍정/중립/부정으로 분류합니다.
 영화 리뷰이나 음식점 리뷰는 데이터의 양이 많고 별점을 함꼐 달기 때문에 긍정/중립/부정 라벨링이 쉬워서 극성 감성 분석에 쉽게 적용할 수 있습니다.
